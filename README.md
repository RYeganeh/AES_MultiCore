فاز دوم پروژه برنامه نویسی چند هسته ای
ریحانه یگانه و زهرا حسینی

تصحیح کد سریال
از آنجا که در کد سریال به زبان c++ در کودا محدودیت هایی داریم و امکان پیاده سازی برخی موارد در این نوع زبان نیست (مانند کلاس و...) ابتدای کار کد را به زبان سی تبدیل کردیم که البته این تغییرات زمانبر و مشکل ساز بود.
موازی سازی در سطح GPU
مواردی که بابت موازی سازی با کودا به کد با توجه به شرایط ایجاد شده است شامل :
نسخه اولیه:
•	استفاده از حافظه constant برای ذخیره کلید های رمزنگاری و s_box  جهت دسترسی سریع تر به داده ها 
•	بازنوبسی توابع برای اجرا روی GPU 
•	استفاده از کرنل برای رمزنگاری چندین بلاک ( در نسخه ابتدایی)
o	در کد سریال بلاک های 16 بایتی به ترتیب پردازش می شدند و برای تعداد بلاک های بالا، اجرای کد بسیار زمان بر میشد. در کد موازی با تعریف یک کرنل برای هر بلاک 16 بایتی یک thread  درGPU  اختصاص دادیم که مسِولیت رمزنگاری آن بلاک را دارد.
•	استفاده از cudaMalloc  و  cudaMemcpy جهت مدیریت حافظه درGPU( درنسخه ابتدایی)
•	استفاده از cudaEvent جهت اندازه گیری زمان اجرای کرنل و مقایسات بعدی 
نسخه دوم:

•	افزودن CUDA Streams
Stream  ها به GPU این اجازه را می دهند که چندین عملیات را به صورت همزمان انجام دهند. در کد اولیه ما صرفا یک کرنل تغریف کرده بودیم که پرازش موازی رو انجام میداد اما در کد جدید با افزودن استریم ها شرایطی رو فراهم کردیم که چندین کرنل به صورت غیر همزمان اجرا شوند. بدین صورت که بلاک ها را بین جریان های تغریف شده (4و8و16) تقسیم کردیم و سپس هر جریان برای بلاک های در اختیار قرار گرفته خودش یک کرنل اجرا می کند. انتقال داده ها فقط در جریان اول انچام شده است. با افزودن استریم ها زمان ما به طور قابل توجهی کاهش پیدا کرد.
•	استفاده از cudaMemcpyAsync

ما از cudaMemcpyAsync استفاده کردیم تا انتقال داده به صورت غیر همزمان انجام شود و GPU بتواند همزمان با انتقال داده ها کرنل ها را نیز اجرا کند این کار باعث کاهش اجرای زمان کلی پردازش می شود.
!! در هنگام موازی سازی از طریق stream، مهم ترین مشکل سربار انجام این کار بود که نیاز به بالا رفتن throughput درآن اهمیت بالاتری پیدا می کرد.
نسخه سوم:
•	پردازش چند بلاک توسط هر رشته
در نسخه پیشین هر رشته تنها یک بلاک را پردازش میکرد. اما در کد جدید تعیین کردیم که هر رشته بتواند چندین بلاک را پردازش کند. این موجب کاهش Grid size در کودا برای تعداد بلاک های بالا می شود و بهره وری را افزایش می دهد.
•	استفاده از Pinned Memory
ما از cudaHostAlloc استفاده کردیم تا بتوانیم از حافظه پین شده استفاده کنیم. این حافظه به دلیل اینکه امکان انتقال مستقیم به GPU را فراهم می کند به اجرای ما سرعت می بخشد. همچنین برای آزادسازی این حافظه از cudaFreeHost استفاده کردیم.
•	بهینه سازی توزیع بلاک ها
در کد جدید ابتدا یک تعداد پایه بلاک را بین استریم ها تقسیم کردیم و سپس بلاک های باقی مانده را بین استریم های اولیه تقسیم کردیم این توزیع نسبت به توزیع قبلی متعادل تر بوده.
•	حذف انتقال داده ها صرفا توسط استریم ابتدایی
در کد جدید هر استریم داده های بلاک های خود را انتقال می دهد این کار موجب افزایش همزمانی می شود.

نسخه چهارم (حافظه مشترک):
برای بررسی تاثیر استفاده از حافظه مشترک بر روی زمان اجرا معماری را کمی تغییر دادیم . چالشی که وجود داشت این بود که GPUها معمولا 48KB حافظه مشترک در هر SM دارند و برای این که به محدودیت های حافظه نخوریم باید به حجم داده مشترک توجه میکردیم. S_box 256 , round_keys 176 بایت بودند و shared_state هم 2048 بایت بود تعداد ترد ها را به 128 کاهش دادیم.
مقایسه زمان های اجرا و انتخاب بهترین نسخه:
زمان اجرای کد سریال:
100 input: Average: 0.973 ms | Average: 1.144 ms | Average: 1.238 ms
1000 input: Average: 14.896 ms | Average: 12.121 ms | Average: 14.711 ms
10000 input: Average: 212.603 ms | Average: 186.329 ms | Average: 172.137 ms
همانطور که مشاهده می شود حدودا از 10000 ورودی به بعد زمان ما به صورت قابل توجهی افزایش می یابد به همین دلیل نقطه شروع را این مورد در نظر گرفتیم.
زمان اجرای کد قبل از بهبود استریمینگ:
10000 input:
4 streams: Average: 2.481 ms | Average: 2.522 ms | Average: 2.497 ms | 
8 streams: Average: 4.938 ms | Average: 4.864 ms | Average: 4.856 ms | 
16 streams: Average: 6.547 ms | 
 ``در این تعداد ورودی مشاهده می شود با افزایش تعداد استریم از 8 به بعد ما سربار داریم و زمان اجرای ما افزایش می یابد.
زمان اجرای کد پس از بهبود استریمینگ:
10000 input:
4 streams: Average: 0.653 ms | Average: 0.685 ms | 
8 streams: Average: 0.723 ms | Average: 0.717 ms | 
16 streams: Average: 0.687 ms | Average: 0.597 ms |
همانطور که مشاهده میشود عملکرد کد به شدت بهبود یافته و زمان های اجرا به طرز قابل توجهی کاهش یافته اند پس از این مرحله به سراغ بررسی تاثیر حافطه ها بر روی زمان اجرا رفتیم. کدی که تا گذشته ران می شد برای حافظه های non_unified طراحی شده بود . کد مجدد مورد بررسی و تصحیح قرار گرفت تا ببینیم در صورت وجود معماری حافظه مشترک چه تاثیری بر زمان اجرا می گذارد.
زمان اجرای کد برای حافطه های مشترک:
10000 input:
4 stream: Average: 0.157 ms | Average: 0.162 ms |
8 stream: Average: 0.212 ms | Average: 0.209 ms | 
16 stream: Average: 0.289 ms | Average: 0.273 ms | 
همانطور که مشاهده می شود زمان اجرا به طور قابل توجهی کاهش یافته است تقریبا زمان اجرا 4/1 شده ایت که نشان می دهد در معماری حافطه مشترک به این دلیل که داده ها یک بار به حافظه مشترک کپی می شوند و دسترسی به آن ها سریع تر است زمان اجرا به طور خاصی کاهش می یابد که البته این مورد برای الگوریتم رمز نگاری ما که ترد ها مکررا از s_box استفاده می کنند تاثیر قابل توجهی گذاشته است.
بررسی تاثیر پارامتر های مختلف در زمان پردازش
Thread Block Size:
این مورد به تعداد ترد ها در هر بلاک کودا مربوط است به معنای دیگر با تنظیم این مورد تعیین میکنیم چند ترد  به صورت همزمان در یک بلاک اجرا شوند.  ما اعداد 64، 128، 192 و 256 را برای آزمایش انتخاب کردیم.
Kernel Granularity:
blocks_per_thread این پارامتر تعیین می کند هر ترد چند بلاک AES را پردازش می کند اگر این مورد و افزایش بدیم تعداد ترد های کمتری نیاز خواهیم داشت اما ممکنه SM ها بیکار بمونن و اگر مقدارش رو خیلی کم انتخاب کنیم باعث سربار میشه چون ترد ها و مدیریتشون زمان بر میشه. ما اعداد 1، 2، 4 و 8 رو برای آزمایش انتخاب کردیم.

اگر تعداد بلاک ها برای هر ترد را افزایش دهیم در نتیجه هر ترد باید به صورت سریال این بلاک ها را پیمایش کند که در نتیجه با افزایش تعداد بلاک هایی که به هر ترد اختصاص می دهیم چون به صورت سریال این مورد د رحال انجام است، زمان ما افزایش می یابد. 
در عین حال اگر تعداد ترد ها در هر بلاک کودا را افزایش دهیم عملکرد بهتر می شود اما این مورد فقط تا عدد 128 به خوبی عمل می کند و بیشتر از آن سربار دارد.
همچنین به دلیل وجود استریم ها، استریم ها را نیز افزایش دادیم و زمان های اجرا را مقایسه کردیم. استریم 16 و 32 زمان های به نسبت بهتری دادند و از استریم 64 ما شاهد افزایش زمان اجرا بودیم که نشان دهنده افزایش سربار بود چون تعداد کمی بلاک به هر استریم می رسید.



